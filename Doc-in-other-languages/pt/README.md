<div class="center" align="center">
  <a href="#">
    <img alt="Py-Crawler-ICON" src="https://helloosdisk.wikidot.com/local--files/file:github/Pyc" width="100px">
  </a><br/>
  <img alt="Py-Crawler-ICON" src="https://helloosdisk.wikidot.com/local--files/file:github/pyctext.png" width="300px">
  <p>Fazer backup do seu wikidot</p>
  <img alt="" src="https://img.shields.io/github/license/HelloOSMe/Py-crawler">&nbsp;&nbsp;<img alt="" src="https://img.shields.io/github/v/release/HelloOSMe/Py-Crawler?include_prereleases">&nbsp;&nbsp;<img alt="" src="https://img.shields.io/github/stars/HelloOSMe/Py-crawler">
</div>

----------------

## Arquivo de código Wiki HTML (Py-crawler)

Isto é usado para criar um projeto crawler que rasteja todas as páginas que existem no wiki para leitura estática - apenas versão do site após o desastre. 
Pode não se aplicar ao projecto de recuperação, mas fornece uma base para restaurar a página. 

Por agora, isto aplica-se a todos os sites da Wikidot, mas para prevenir rastejadores maliciosos, qualquer site deve ter uma página para escalar.

### **Requisitos de configuração**

```
Crie uma página no URL/páginas do seu site e adicione [[module Pages preview="true"]] código
```

### **Instalação do rastreador**

Por favor, clique na versão mais recente exibida na barra lateral 'Releases' e selecione "Source Code" para baixar.

### **O rastejador corre**
* Habilitar PyCrawlerGUI.exe em tempo de execução.
* Insira seu URL no URL e clique em OK.
* Vá para verificar o próximo número de página X de X (neste exemplo, o próximo x) exibido sob o URL/sistema: list-all-pages page do seu site
* Digite este número nas páginas [se você preencher mais de um (defina este número como X, X ≠ 0), ele irá produzir 1~X arquivos HTML (nome: pages1~pagesX. html), mas (X-1) e X são os mesmos], e então wrap.
* Aguarde silenciosamente até que o programa termine a execução. Haverá um prompt pop-up.
* Terminado? Lembre-se de mover a pasta html para evitar confusão.

### **Captura de Ecrã**
<img alt="" src="https://s1.ax1x.com/2023/02/20/pSXVpQJ.jpg" width="300px">  
<img alt="" src="https://s1.ax1x.com/2023/02/20/pSXExWF.jpg" width="300px">  
<img alt="" src="https://s1.ax1x.com/2023/02/20/pSXEzz4.jpg" width="300px">

### **Endereço de feedback**
TEM UM ERRO OU ESCUTA? Venha para o feedback [aqui](http://ld-private-website.wikidot.com/forum/c-7602918/pyc) ou venha [aqui](https://github.com/HelloOSMe/Py-crawler/issues) feedback.

Tem uma ideia para uma nova funcionalidade, mas não sabe onde dar feedback? Venha para o feedback [aqui](http://ld-private-website.wikidot.com/forum/t-15402049/pyc-1-1-0-1-9) ou venha [aqui](https://github.com/HelloOSMe/Py-crawler/issues) feedback.

Pode dar-nos ajuda técnica? Venha para o feedback [aqui](http://ld-private-website.wikidot.com/forum/c-7602920/), ou [aqui](https://github.com/HelloOSMe/Py-crawler/fork) puxe ramos para fazer alterações.

----------

```
Copyright (c) 2023 Py-crawler Dev Team
All Rights Reserved.

Copyright (c) 2023 Py- crawler Dev Team
Todos os direitos reservados.

O ícone "Py-Crawler-worm" é feito por hatoyama_kumiko e é aplicável ao protocolo CC-BY-SA-4.0.

A aplicação feita pelo RedPanda Dev-Cpp , Visual Basic 6.0 e Windows Notepad.
```
