<div class="center" align="center">
  <a href="#">
    <img alt="Py-Crawler-ICON" src="https://rmlsdisk.wikidot.com/local--files/file:github/Pyc" width="100px">
  </a><br/>
  <img alt="Py-Crawler-ICON" src="https://rmlsdisk.wikidot.com/local--files/file:github/pyctext.png" width="300px">
  <p>Copia de Seguridad de tu wikidot</p>
  <img alt="" src="https://img.shields.io/github/license/Remelens/WD-crawler">&nbsp;&nbsp;<img alt="" src="https://img.shields.io/github/v/release/Remelens/WD-Crawler?include_prereleases">&nbsp;&nbsp;<img alt="" src="https://img.shields.io/github/stars/Remelens/WD-crawler">
</div>

----------
## WIKI HTML-kodarkivo (Py crawler)

Esto se utiliza para crear un proyecto de rastreador para arrastrar todas las páginas presentes en el wiki para versiones estáticas de solo lectura del sitio web después del desastre. Puede que no se aplique a las obras de restauración, pero proporciona una base para restaurar la página.

Por el momento, esto se aplica a todos los sitios web de wikidot, pero para evitar reptiles maliciosos, cualquier sitio web debe tener una página /system:list-all-pages para subir.

### **Agordo- bezonoj**
```
Krei paĝon sur via retpaĝo URL/paĝoj kaj aldoni [[module Pages preview="true"]] kodon
```

### **Programinstalo**

Haga clic en la última versión mostrada en la barra lateral `releases` y elija' código fuente 'para descargarla.

### **Raupenbetrieb**
* habilitar pycrawlergui.exe en tiempo de ejecución.

* introduzca su sitio web en la dirección web y haga clic en confirmar.

* ve a ver el último número de la página X de X que se muestra debajo de la página de tu sitio web url / system: list - all - pages (en este ejemplo es la última x)

* introduzca este número en las páginas [si rellena más (deje que este número sea x, x ÷ 0), exportará archivos htl de 1 a X (nombre pages1 a pagesx.html) pero (x - 1) y X son los mismos] y luego cambie de línea.

* espera en silencio a que el programa se complete, habrá un recordatorio de ventana emergente.

¿¿ se ha completado? Recuerda mover la carpeta html para evitar la confusión.

### **Captura de pantalla**
<img alt="" src="https://s1.ax1x.com/2023/02/20/pSXVpQJ.jpg" width="300px">  
<img alt="" src="https://s1.ax1x.com/2023/02/20/pSXExWF.jpg" width="300px">  
<img alt="" src="https://s1.ax1x.com/2023/02/20/pSXEzz4.jpg" width="300px">

### **Comment**

@ title: window Venu ĉi tie. [Aŭ venu ĉi tie](https://github.com/Remelens/WD-crawler/issues).

Ĉu havas novajn funkciajn ideojn, sed ne scias, kie resti? Venu ĉi tie. [Aŭ venu ĉi tie](https://github.com/Remelens/WD-crawler/issues).

Ĉu vi povas provi al ni teknikan helpon? Venu ĉi tie. [Aŭ venu ĉi tie](https://github.com/Remelens/WD-crawler/fork).

----------

```
Copyright (c) 2022-2023 Remelens
All Rights Reserved.

Copyright (c) 2022-2023 Remelens
Ĉiuj rajtoj rezervitaj.

El icono "py crawler worm" está hecho por Hatoyama Kumiko y se aplica al Protocolo CC-by-SA-4.0.

La aplicación fue producida por redpanda Dev-cpp, Visual Basic 6.0 y Windows notebook.
```
