<div class="center" align="center">
  <a href="#">
    <img alt="Py-Crawler-ICON" src="https://helloosdisk.wikidot.com/local--files/file:github/Pyc" width="100px">
  </a><br/>
  <img alt="Py-Crawler-ICON" src="https://helloosdisk.wikidot.com/local--files/file:github/pyctext.png" width="300px">
  <p>Sichern Sie Ihren Wikidot</p>
  <img alt="" src="https://img.shields.io/github/license/HelloOSMe/Py-crawler">&nbsp;&nbsp;<img alt="" src="https://img.shields.io/github/v/release/HelloOSMe/Py-Crawler?include_prereleases">&nbsp;&nbsp;<img alt="" src="https://img.shields.io/github/stars/HelloOSMe/Py-crawler">
</div>

----------

## Wiki HTML Code Archiv (Py-Crawler)

Dies wird verwendet, um ein Crawler-Projekt zum Crawlen aller im Wiki vorhandenen Seiten und für statische schreibgeschützte Versionen der Website nach der Katastrophe zu erstellen. Sie ist möglicherweise nicht auf das Wiederherstellungsprojekt anwendbar, bietet aber eine Grundlage für die Wiederherstellungsseite.

Derzeit gilt dies für alle Wikidot-Websites, aber um bösartige Crawler zu verhindern, muss jede Website ein/system haben: listet alle Seiten auf, die gecrawlt werden sollen.

### **Konfigurationsanforderungen**

```
Erstellen Sie eine Seite auf Ihrer Website URL/System: list-all-pages und fügen Sie [[module Pages preview="true"]] Code hinzu
```

### **Raupeninstallation**
Bitte klicken Sie auf die neueste Version in der Seitenleiste `Releases` und wählen Sie "Source Code" zum Herunterladen.

### **Raupenbetrieb**
* Führen Sie `./main` im Projektstammverzeichnis mit bash zur Laufzeit aus.
* Geben Sie Ihre Website-Adresse an der URL ein und führen Sie diese als neue Zeile aus.
* Überprüfen Sie die nächste Anzahl der Seite X von X (in diesem Beispiel das nächste x), die unter der URL/system: list-all-pages Seite Ihrer Website angezeigt wird.
* Geben Sie diese Zahl in Seiten ein [wenn Sie mehr als eine ausfüllen (setzen Sie diese Zahl auf X, X ≠ 0), es werden 1~X HTML Dateien ausgegeben (Name: pages1~pagesX. html), aber (X-1) und X sind gleich), und dann umbrechen.
* Warten Sie ruhig, bis das Programm beendet ist. Es wird eine Popup-Aufforderung angezeigt.
* Fertig? Denken Sie daran, den html-Ordner zu verschieben, um Verwirrung zu vermeiden.

### **Screenshot**
<img alt="" src="https://s1.ax1x.com/2023/02/20/pSXVpQJ.jpg" width="300px">  
<img alt="" src="https://s1.ax1x.com/2023/02/20/pSXExWF.jpg" width="300px">  
<img alt="" src="https://s1.ax1x.com/2023/02/20/pSXEzz4.jpg" width="300px">

### **Feedback-Adresse**


HABEN SIE EINEN FEHLER ODER BUG? Kommen Sie zu [hier](https://github.com/HelloOSMe/Py-crawler/issues) Feedback.

Sie haben eine Idee für ein neues Feature, wissen aber nicht, wo Sie Feedback geben sollen? Kommen Sie zu [hier](https://github.com/HelloOSMe/Py-crawler/issues) Feedback.

Können Sie uns technische Hilfe geben? Holen Sie sich [hier](https://github.com/HelloOSMe/Py-crawler/fork) Zweige, um Änderungen vorzunehmen.

----------

```
Copyright (c) 2022-2023 HelloOSMe
All Rights Reserved.

Copyright (c) 2022-2023 HelloOSMe
Alle Rechte vorbehalten.

Die Anwendung für Linux gemacht von RedPanda Dev-Cpp Replit und Windows Notepad.
```
