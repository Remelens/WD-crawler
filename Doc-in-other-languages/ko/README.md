<div class="center" align="center">
  <a href="#">
    <img alt="Py-Crawler-ICON" src="https://helloosdisk.wikidot.com/local--files/file:github/Pyc" width="100px">
  </a><br/>
  <img alt="Py-Crawler-ICON" src="https://helloosdisk.wikidot.com/local--files/file:github/pyctext.png" width="300px">
  <p>Wikidot 백업</p>
  <img alt="" src="https://img.shields.io/github/license/HelloOSMe/Py-crawler">&nbsp;&nbsp;<img alt="" src="https://img.shields.io/github/v/release/HelloOSMe/Py-Crawler?include_prereleases">&nbsp;&nbsp;<img alt="" src="https://img.shields.io/github/stars/HelloOSMe/Py-crawler">
</div>

---------------

## 위키HTML 코드 아카이브(Py-crawler)

이것은 영향을받는 웹 사이트의 정적 읽기 전용 버전에 대한 위키에 존재하는 모든 페이지를 크롤링하는 크롤링 프로젝트를 만드는 데 사용됩니다. 복구 작업에는 적용되지 않을 수 있지만 페이지를 복원할 수 있는 기반을 제공합니다.
현재 모든 위키닷 웹 사이트에 적용되지만 악의적인 크롤러를 방지하기 위해 모든 웹 사이트에는 크롤링할 수 있는 페이지 페이지가 있어야 합니다.

### **요구 사항을 구성합니다**

```
웹 사이트 URL/pages에 페이지를 만들고 [[module Pages preview="true"]] 코드를 추가합니다
```

### **파충류 프로그램 설치**

사이드바`Releases`에 표시된 최신 버전을 클릭하고'Source Code'를 선택하여 다운로드하십시오.

### **파충류가 실행됩니다**
* 런타임 시 bash를 사용하여 프로젝트 루트에서 `./main`。
* URL에 웹 주소를 입력하고 줄을 바꾸어 실행합니다.
* 웹 사이트 URL/system을 확인하십시오: list-all-pages 페이지 아래에 표시된 page X of X의 다음 수 (이 예에서는 다음 x)
* pages에 이 수를 입력하십시오 [이 수를 X로 설정하면 (이 수를 X, X ≠ 0) 1~X의 HTML 파일 (이름은 pages1~pagesX.html) 을 출력하지만 (X-1) 과 X는 동일합니다] 를 입력하고 줄을 바꿉니다.
* 프로그램이 실행될 때까지 조용히 기다리세요. 탄창 알림이 있을 거예요.
* 완료되었습니까?html 폴더를 이동해서 혼동을 피하는 것을 잊지 마세요.

### **피드백 주소입니다**
오류 또는 BUG가 발생 되었거나? [여기](https://github.com/HelloOSMe/Py-crawler/issues) 피드백에 오십시오.

새로운 기능에 대한 아이디어가 있지만 피드백이 어디에 있는지 모르습니까? [여기](https://github.com/HelloOSMe/Py-crawler/issues) 피드백에 오십시오.

당신은 우리에게 기술 지원을 제공 할 수 있습니까? [여기](https://github.com/HelloOSMe/Py-crawler/fork)로 가서 분기를 당겨 수정합니다.

----------

```
Copyright (c) 2022-2023 HelloOSMe
All Rights Reserved.

저작권 (c) 2022-2023 HelloOSMe

모든 권리를 보유하다.RedPanda Dev Cpp, Replit 및 Windows 메모장으로 제작된 Linux 애플리케이션입니다.
```
