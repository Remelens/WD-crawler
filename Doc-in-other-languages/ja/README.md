<div class="center" align="center">
  <a href="#">
    <img alt="Py-Crawler-ICON" src="https://helloosdisk.wikidot.com/local--files/file:github/Pyc" width="100px">
  </a><br/>
  <img alt="Py-Crawler-ICON" src="https://helloosdisk.wikidot.com/local--files/file:github/pyctext.png" width="300px">
  <p>あなたのwikidotをバックアップします</p>
  <img alt="" src="https://img.shields.io/github/license/HelloOSMe/Py-crawler">&nbsp;&nbsp;<img alt="" src="https://img.shields.io/github/v/release/HelloOSMe/Py-Crawler?include_prereleases">&nbsp;&nbsp;<img alt="" src="https://img.shields.io/github/stars/HelloOSMe/Py-crawler">
</div>

----------

## ウィキHTMLコードアーカイブ(Py-crawler)

これは、Wiki 上に存在するすべてのページをクロールするクローラ プロジェクトを作成し、被災後の Web サイトの静的読み取り専用バージョンに使用されます。 回復プロジェクトには適用されない場合がありますが、回復ページの基礎となります。
現時点では、これはすべてのWikidotサイトで動作しますが、悪意のあるクローラを防ぐために、任意のウェブサイトは、クロールする前にpagesページを持っている必要があります。

### **構成要件**
```
Web サイトの URL/pages にページを作成し、" [[module Pages preview="true"]] " コードを追加します
```

### **爬虫類プログラムのインストール**

サイドバー`Releases`に表示されている最新版をクリックし、「Source Code」を選択してダウンロードしてください。

### **爬虫類運転**
* 実行時にbashを使用してプロジェクトルートの下で実行`./main`。
* URLにURLを入力し、改行して実行します。
* あなたのウェブサイトURL/system：list-all-pagesページの下に表示されているpage X of Xの後ろの数を見てください（この例では後ろのxです）
* pagesにこの数を入力して（この数をX、X≠0とする）、1～XのHTMLファイル（pages 1～pagesX.htmlという名前）を出力しますが（X-1）とXは同じです）、改行します。
* プログラムが実行されるのを静かに待ちましょう。ポップアップが表示されます。
* 完了しましたか？htmlフォルダを混同しないように移動してください。

### **フィードバック アドレス**
エラーまたはバグが発生しましたか? [ここ](https://github.com/HelloOSMe/Py-crawler/issues)!

新機能のアイデアがありますが、フィードバックがわからない? [ここ](https://github.com/HelloOSMe/Py-crawler/issues)フィードバックに来てください。

あなたは私たちに技術的な助けを与えることができますか? [ここ](https://github.com/HelloOSMe/Py-crawler/fork)に来て、ブランチを引っ張って修正してください。

----------
```
Copyright (c) 2022-2023 HelloOSMe
All Rights Reserved.

著作権所有 (c) 2022-2023 HelloOSMe
すべての権利を保持します。

RedPanda Dev Cpp、Replit、Windowsメモ帳から作成されたLinuxアプリケーション。
```
